{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 联邦学习简单实现 —— FedAvg 算法\n",
    "\n",
    "本 Notebook 实现了一个简单的联邦学习示例。我们将使用 MNIST 数据集，将训练数据分成多个客户端，每个客户端在本地训练一个简单的神经网络（MLP），然后采用 FedAvg 算法对各客户端的模型参数进行平均聚合，更新全局模型。整个过程分为数据加载与划分、模型定义、局部训练、模型聚合和全局评估等步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# 超参数设置\n",
    "num_clients = 5        # 客户端数量\n",
    "local_epochs = 1       # 每个客户端本地训练的轮数\n",
    "batch_size = 32        # 批次大小\n",
    "num_rounds = 5         # 联邦学习的通信轮数\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_classes = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与客户端数据划分\n",
    "我们使用 MNIST 数据集，并将训练数据均匀划分给多个客户端。每个客户端拥有自己独立的数据子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理：将图片转换为 tensor 并归一化\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 下载 MNIST 数据集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 将训练集均分给各个客户端（这里简单采用均分的方式）\n",
    "#client_data_sizes = [len(train_dataset) // num_clients] * num_clients\n",
    "#client_datasets = random_split(train_dataset, client_data_sizes)\n",
    "#client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Dirichlet 分布进行非 IID 数据划分并可视化分布情况\n",
    "\n",
    "下面的代码实现了一个 `dirichlet_partition` 函数，该函数：\n",
    "\n",
    "1. 固定随机种子（同时设置了 numpy、random、torch 的 seed），保证每次运行结果一致。\n",
    "2. 对数据集按照类别进行划分：对于每个类别，从 Dirichlet 分布中采样各客户端分配比例，然后将该类别样本按照比例分配到各个客户端。\n",
    "3. 划分结果生成后，我们统计各客户端中各类别的样本数，并绘制堆叠柱状图展示数据分布情况。\n",
    "\n",
    "你可以调整 `alpha` 参数来控制各客户端数据分布的“非 IID 程度”（alpha 越小，各客户端类别分布越不均衡）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def dirichlet_partition(dataset, n_clients, alpha, seed=42):\n",
    "    \"\"\"\n",
    "    使用 Dirichlet 分布将数据集划分给 n_clients 个客户端，保证每次运行结果相同。\n",
    "    \n",
    "    参数:\n",
    "        dataset: torch 数据集，要求 dataset.targets 为 tensor 类型的标签\n",
    "        n_clients: 客户端数量\n",
    "        alpha: Dirichlet 分布的参数，控制数据非 IID 程度，alpha 越小分布越极端\n",
    "        seed: 随机种子，保证结果可重复\n",
    "        \n",
    "    返回:\n",
    "        client_subsets: 长度为 n_clients 的 Subset 列表，每个 Subset 表示分配给对应客户端的数据\n",
    "    \"\"\"\n",
    "    # 固定随机种子，保证结果可复现\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # 获取所有标签（假设 dataset.targets 为 tensor 类型）\n",
    "    targets = dataset.targets.numpy()\n",
    "    n_classes = len(np.unique(targets))\n",
    "    \n",
    "    # 存储每个客户端的样本索引\n",
    "    client_indices = {i: [] for i in range(n_clients)}\n",
    "    \n",
    "    # 针对每个类别进行划分\n",
    "    for cls in range(n_classes):\n",
    "        cls_idx = np.where(targets == cls)[0]\n",
    "        # 打乱该类别样本顺序\n",
    "        np.random.shuffle(cls_idx)\n",
    "        # 从 Dirichlet 分布中采样各客户端分配比例\n",
    "        proportions = np.random.dirichlet(alpha * np.ones(n_clients))\n",
    "        # 根据比例计算每个客户端该类别样本数量的分割点\n",
    "        proportions = (np.cumsum(proportions) * len(cls_idx)).astype(int)[:-1]\n",
    "        cls_idx_split = np.split(cls_idx, proportions)\n",
    "        for i, indices in enumerate(cls_idx_split):\n",
    "            client_indices[i].extend(indices.tolist())\n",
    "    \n",
    "    # 根据索引生成 Subset\n",
    "    client_subsets = [Subset(dataset, client_indices[i]) for i in range(n_clients)]\n",
    "    return client_subsets\n",
    "\n",
    "# 使用示例\n",
    "alpha = 0.1    # Dirichlet 参数，可根据需要调整\n",
    "n_clients = 5  # 客户端数量\n",
    "seed = 42      # 固定随机种子\n",
    "\n",
    "# 假设 train_dataset 已经定义并下载了 MNIST 数据集\n",
    "client_datasets = dirichlet_partition(train_dataset, n_clients, alpha, seed)\n",
    "client_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in client_datasets]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 可视化各客户端的标签分布\n",
    "#\n",
    "# 下面统计每个客户端中各类别（0～9）的样本数量，并绘制堆叠柱状图展示分布情况。\n",
    "\n",
    "# %%\n",
    "n_classes = 10  # MNIST 有 10 个类别\n",
    "client_class_counts = []\n",
    "\n",
    "for client_subset in client_datasets:\n",
    "    # 获取当前客户端的样本索引及对应标签\n",
    "    indices = client_subset.indices\n",
    "    labels = train_dataset.targets[indices].numpy()\n",
    "    counts = [np.sum(labels == i) for i in range(n_classes)]\n",
    "    client_class_counts.append(counts)\n",
    "\n",
    "client_class_counts = np.array(client_class_counts)  # shape: (n_clients, n_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "client_ids = np.arange(n_clients)\n",
    "bottom = np.zeros(n_clients)\n",
    "colors = plt.cm.tab10.colors  # 10种颜色\n",
    "\n",
    "for cls in range(n_classes):\n",
    "    plt.bar(client_ids, client_class_counts[:, cls], bottom=bottom, \n",
    "            color=colors[cls], label=f\"Class {cls}\")\n",
    "    bottom += client_class_counts[:, cls]\n",
    "\n",
    "plt.xlabel(\"Client ID\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(f\"Label Distribution per Client (Dirichlet Partition, alpha={alpha})\")\n",
    "plt.xticks(client_ids, [f\"Client {i}\" for i in client_ids])\n",
    "plt.legend(title=\"Class\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "这里我们定义一个简单的全连接神经网络（MLP），用于 MNIST 手写数字分类。模型由一个隐藏层构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 将 28x28 的图像展平为 784 维向量\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部训练函数\n",
    "定义每个客户端的本地训练过程。每个客户端在自己的数据上训练若干个 epoch，并返回训练后的模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_train(model, dataloader, epochs, device):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # 返回训练后的模型参数\n",
    "    return model.state_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FedAvg 聚合函数（数据量加权）\n",
    "\n",
    "在实际应用中，各客户端的数据量可能不一致，因此在聚合时需要根据每个客户端的数据量进行加权：\n",
    "$$\n",
    "\\theta_{global} = \\sum_i \\frac{n_i}{N_{total}} \\theta_i\n",
    "$$\n",
    "其中$ n_i $是客户端$ i $的数据量，$ N_{total} $是所有客户端数据量之和。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(global_model, client_state_dicts, client_data_counts):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    # 初始化全局模型参数为零\n",
    "    for key in global_state_dict.keys():\n",
    "        global_state_dict[key] = torch.zeros_like(global_state_dict[key])\n",
    "    total_samples = sum(client_data_counts)\n",
    "    # 对每个客户端的参数按数据量权重累加\n",
    "    for client_state, n_samples in zip(client_state_dicts, client_data_counts):\n",
    "        weight = n_samples / total_samples\n",
    "        for key in global_state_dict.keys():\n",
    "            global_state_dict[key] += client_state[key] * weight\n",
    "    # 更新全局模型\n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "    return global_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 联邦学习主流程及每轮局部模型测试\n",
    "在每一轮全局通信中，我们让每个客户端先在自己的本地训练后：\n",
    "- 使用其本地模型在整个测试集上进行评估（计算总体准确率和各类别准确率）。\n",
    "- 最后，将所有客户端的模型按照各自数据量的权重使用 FedAvg 进行聚合，形成更新后的全局模型。\n",
    "\n",
    "这样可以帮助我们观察每个客户端在局部训练后的性能，同时也能追踪全局模型在聚合后的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, device, test_loader, n_classes=10):\n",
    "    \"\"\"\n",
    "    在测试集上评估给定模型的表现，返回总体准确率及各类别准确率。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0 for _ in range(n_classes)]\n",
    "    class_total = [0 for _ in range(n_classes)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            for i in range(target.size(0)):\n",
    "                label = target[i].item()\n",
    "                class_total[label] += 1\n",
    "                if predicted[i].item() == label:\n",
    "                    class_correct[label] += 1\n",
    "                    \n",
    "    overall_accuracy = 100.0 * correct / total\n",
    "    class_accuracies = [100.0 * c / t if t > 0 else 0.0 for c, t in zip(class_correct, class_total)]\n",
    "    return overall_accuracy, class_accuracies\n",
    "\n",
    "# 定义测试数据加载器\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 联邦学习主流程（使用数据量加权的 FedAvg）\n",
    "这里首先计算每个客户端的数据量，然后在每一轮通信中：\n",
    "\n",
    "- 将全局模型下发到各客户端进行本地训练；\n",
    "- 收集各客户端更新后的模型参数；\n",
    "- 使用上述加权 FedAvg 方法聚合各客户端模型参数，更新全局模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个客户端数据量\n",
    "client_data_counts = [len(dataset) for dataset in client_datasets]\n",
    "print(\"每个客户端数据量:\", client_data_counts)\n",
    "\n",
    "global_model = SimpleNN().to(device)\n",
    "\n",
    "\n",
    "for r in range(num_rounds):\n",
    "    print(f\"==== 第 {r+1} 轮通信 ====\")\n",
    "    client_state_dicts = []\n",
    "    local_models = []\n",
    "    \n",
    "    # 遍历每个客户端，进行本地训练和测试\n",
    "    for c_id, client_loader in enumerate(client_loaders):\n",
    "        local_model = SimpleNN().to(device)\n",
    "        # 同步全局模型参数到客户端\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        # 客户端本地训练\n",
    "        local_state = local_train(local_model, client_loader, local_epochs, device)\n",
    "        client_state_dicts.append(local_state)\n",
    "        local_models.append(local_model)\n",
    "        \n",
    "        # 测试当前客户端本地模型在整个测试集上的表现\n",
    "        overall_acc, class_acc = evaluate_model(local_model, device, test_loader, n_classes)\n",
    "        print(f\"Client {c_id} local model test accuracy: {overall_acc:.2f}%\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"  Class {i} accuracy: {class_acc[i]:.2f}%\")\n",
    "    \n",
    "    # 使用数据量加权的 FedAvg 聚合各客户端模型\n",
    "    global_model = fed_avg(global_model, client_state_dicts, client_data_counts)\n",
    "    \n",
    "    # 选择性：测试聚合后的全局模型\n",
    "    global_acc, global_class_acc = evaluate_model(global_model, device, test_loader, n_classes)\n",
    "    print(f\"Global model after round {r+1} test accuracy: {global_acc:.2f}%\")\n",
    "    for i in range(n_classes):\n",
    "        print(f\"  Global model Class {i} accuracy: {global_class_acc[i]:.2f}%\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
